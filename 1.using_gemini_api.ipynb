{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for reference :\n",
    "\n",
    "#langchain_core.documents.base.Document¶ documenetaion\n",
    "#https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html\n",
    "\n",
    "\n",
    "#for langchain_core.prompts.prompt.PromptTemplate  Documenetaion\n",
    "#https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For langchain_core:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-core in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (0.1.46)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langchain-core) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langchain-core) (0.1.51)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core) (2.3.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core) (8.2.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core) (3.10.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1->langchain-core) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1->langchain-core) (2.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from pydantic<3,>=1->langchain-core) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For langchain_community:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (0.0.34)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langchain-community) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from langchain-community) (3.9.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from langchain-community) (0.6.3)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.45 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langchain-community) (0.1.46)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langchain-community) (0.1.51)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langchain-community) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from langchain-community) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<0.2.0,>=0.1.45->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<0.2.0,>=0.1.45->langchain-community) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<0.2.0,>=0.1.45->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3,>=2->langchain-community) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.45->langchain-community) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.45->langchain-community) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.45->langchain-community) (2.6.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For langchain_text_splitters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-text-splitters in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.28 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langchain-text-splitters) (0.1.46)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (0.1.51)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2.3.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (8.2.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (3.10.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-text-splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For langchain_google_genai:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-google-genai\n",
      "  Downloading langchain_google_genai-1.0.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: google-generativeai<0.6.0,>=0.5.0 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langchain-google-genai) (0.5.2)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.27 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langchain-google-genai) (0.1.46)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.2 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (0.6.2)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (2.18.0)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (2.126.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (2.21.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (4.23.4)\n",
      "Requirement already satisfied: pydantic in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (2.3.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (4.11.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from google-ai-generativelanguage==0.6.2->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (1.23.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langchain-core<0.2,>=0.1.27->langchain-google-genai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<0.2,>=0.1.27->langchain-google-genai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langchain-core<0.2,>=0.1.27->langchain-google-genai) (0.1.51)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<0.2,>=0.1.27->langchain-google-genai) (23.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<0.2,>=0.1.27->langchain-google-genai) (8.2.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from google-api-core->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (1.63.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from google-api-core->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (1.26.18)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.27->langchain-google-genai) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2,>=0.1.27->langchain-google-genai) (3.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from pydantic->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\users\\poppo\\appdata\\roaming\\python\\python310\\site-packages (from pydantic->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (2.6.3)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (4.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from tqdm->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.2->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (1.62.2)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.2->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (1.62.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (3.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.6.0,>=0.5.0->langchain-google-genai) (2024.2.2)\n",
      "Downloading langchain_google_genai-1.0.2-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: langchain-google-genai\n",
      "Successfully installed langchain-google-genai-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement PDFMinerLoader (from versions: none)\n",
      "ERROR: No matching distribution found for PDFMinerLoader\n"
     ]
    }
   ],
   "source": [
    "!pip install PDFMinerLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from pdfminer.six) (3.3.2)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six)\n",
      "  Using cached cryptography-42.0.5-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer.six)\n",
      "  Using cached cffi-1.16.0-cp310-cp310-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.6 MB 435.7 kB/s eta 0:00:13\n",
      "   ---------------------------------------- 0.1/5.6 MB 544.7 kB/s eta 0:00:11\n",
      "    --------------------------------------- 0.1/5.6 MB 798.9 kB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.2/5.6 MB 1.0 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.3/5.6 MB 1.4 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.5/5.6 MB 1.7 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.7/5.6 MB 2.2 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.0/5.6 MB 2.5 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.2/5.6 MB 2.8 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.4/5.6 MB 3.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.6/5.6 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.7/5.6 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.7/5.6 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.7/5.6 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.7/5.6 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.7/5.6 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 2.7/5.6 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.8/5.6 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.0/5.6 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.3/5.6 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.5/5.6 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.7/5.6 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.0/5.6 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.2/5.6 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 4.4/5.6 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.6/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.9/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.1/5.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.3/5.6 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.6 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 3.9 MB/s eta 0:00:00\n",
      "Using cached cryptography-42.0.5-cp39-abi3-win_amd64.whl (2.9 MB)\n",
      "Using cached cffi-1.16.0-cp310-cp310-win_amd64.whl (181 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: pycparser, cffi, cryptography, pdfminer.six\n",
      "Successfully installed cffi-1.16.0 cryptography-42.0.5 pdfminer.six-20231228 pycparser-2.22\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from nltk) (2024.4.16)\n",
      "Requirement already satisfied: tqdm in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\poppo\\anaconda3\\envs\\ai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI API key\n",
    "with open('gemini_key.txt') as f: \n",
    "    KEY = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyCFsIoNRIQytBpUIifvPJbyQXCkFD8tMRU\n"
     ]
    }
   ],
   "source": [
    "print(KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chat prompt template\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"\"\"Hey there! I'm your friendly AI assistant. \n",
    "    I'm here to provide you with insightful answers based on your questions and context.\"\"\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"\"\"Let's uncover some insights together!\n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Your Question: \n",
    "    {question}\n",
    "    \n",
    "    My Response: \"\"\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google Generative AI model\n",
    "chat_model = ChatGoogleGenerativeAI(google_api_key=KEY, model=\"gemini-1.5-pro-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output parser\n",
    "output_parser = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PDF loader\n",
    "pdf_loader = PDFMinerLoader(r\"paper.pdf\")\n",
    "pdf_docs = pdf_loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Preprint.Underreview.LeaveNoContextBehind:Ef\\ue000cientIn\\ue000niteContextTransformerswithIn\\ue000ni-attentionTsendsurenMunkhdalai,ManaalFaruquiandSiddharthGopalGoogletsendsuren@google.comAbstractThisworkintroducesanef\\ue000cientmethodtoscaleTransformer-basedLargeLanguageModels(LLMs)toin\\ue000nitelylonginputswithboundedmemoryandcomputation.Akeycomponentinourproposedapproachisanewat-tentiontechniquedubbedIn\\ue000ni-attention.TheIn\\ue000ni-attentionincorporatesacompressivememoryintothevanillaattentionmechanismandbuildsinbothmaskedlocalattentionandlong-termlinearattentionmechanismsinasingleTransformerblock.Wedemonstratetheeffectivenessofourapproachonlong-contextlanguagemodelingbenchmarks,1Msequencelengthpasskeycontextblockretrievaland500Klengthbooksummarizationtaskswith1Band8BLLMs.OurapproachintroducesminimalboundedmemoryparametersandenablesfaststreaminginferenceforLLMs.1IntroductionMemoryservesasacornerstoneofintelligence,asitenablesef\\ue000cientcomputationstailoredtospeci\\ue000ccontexts.However,Transformers(Vaswanietal.,2017)andTransformer-basedLLMs(Brownetal.,2020;Touvronetal.,2023;Aniletal.,2023;Groeneveldetal.,2024)haveaconstrainedcontext-dependentmemory,duetothenatureoftheattentionmechanism.Figure1:In\\ue000ni-attentionhasanaddi-tionalcompressivememorywithlinearattentionforprocessingin\\ue000nitelylongcontexts.{KV}s−1and{KV}sareatten-tionkeyandvaluesforcurrentandprevi-ousinputsegments,respectivelyandQstheattentionqueries.PEdenotespositionembeddings.TheattentionmechanisminTransformersex-hibitsquadraticcomplexityinbothmemoryfootprintandcomputationtime.Forexample,theattentionKey-Value(KV)stateshave3TBmemoryfootprintfora500Bmodelwithbatchsize512andcontextlength2048(Popeetal.,2023).Indeed,scalingLLMstolongersequences(i.e.1Mtokens)ischallengingwiththestandardTransformerarchitecturesandservinglongerandlongercontextmodelsbecomescostly\\ue000nan-cially.Compressivememorysystemspromisetobemorescalableandef\\ue000cientthantheattentionmechanismforextremelylongsequences(Kan-erva,1988;Munkhdalaietal.,2019).Insteadofusinganarraythatgrowswiththeinputse-quencelength,acompressivememoryprimarilymaintainsa\\ue000xednumberofparameterstostoreandrecallinformationwithaboundedstorageandcomputationcosts.Inthecompressivemem-ory,newinformationisaddedtothememorybychangingitsparameterswithanobjectivethatthisinformationcanberecoveredbacklateron.However,theLLMsintheircurrentstatehaveyettoseeaneffective,practicalcompres-sivememorytechniquethatbalancessimplicityalongwithquality.1arXiv:2404.07143v1  [cs.CL]  10 Apr 2024\\x0cPreprint.Underreview.Inthiswork,weintroduceanovelapproachthatenablesTransformerLLMstoeffectivelyprocessin\\ue000nitelylonginputswithboundedmemoryfootprintandcomputation.AkeycomponentinourproposedapproachisanewattentiontechniquedubbedIn\\ue000ni-attention(Figure1).TheIn\\ue000ni-attentionincorporatesacompressivememoryintothevanillaattentionmechanism(Bahdanauetal.,2014;Vaswanietal.,2017)andbuildsinbothmaskedlocalattentionandlong-termlinearattentionmechanismsinasingleTransformerblock.Suchasubtlebutcriticalmodi\\ue000cationtotheTransformerattentionlayerenablesanaturalextensionofexistingLLMstoin\\ue000nitelylongcontextsviacontinualpre-trainingand\\ue000ne-tuning.OurIn\\ue000ni-attentionreusesallthekey,valueandquerystatesofthestandardattentioncomputationforlong-termmemoryconsolidationandretrieval.WestoreoldKVstatesoftheattentioninthecompressivememory,insteadofdiscardingthemlikeinthestandardattentionmechanism.Wethenretrievethevaluesfromthememorybyusingtheattentionquerystateswhenprocessingsubsequentsequences.Tocomputethe\\ue000nalcontextualoutput,theIn\\ue000ni-attentionaggregatesthelong-termmemory-retrievedvaluesandthelocalattentioncontexts.Inourexperiments,weshowthatourapproachoutperformsbaselinemodelsonlong-contextlanguagemodelingbenchmarkswhilehaving114xcomprehensionratiointermsofmemorysize.Themodelachievesevenbetterperplexitywhentrainedwith100Ksequencelength.A1BLLMnaturallyscalesto1MsequencelengthandsolvesthepasskeyretrievaltaskwheninjectedwithIn\\ue000ni-attention.Finally,weshowthata8BmodelwithIn\\ue000ni-attentionreachesanewSOTAresultona500Klengthbooksummarizationtaskaftercontinualpre-trainingandtask\\ue000ne-tuning.Insummary,ourworkmakesthefollowingcontributions:1.Weintroduceapracticalandyetpowerfulattentionmechanism–In\\ue000ni-attentionwithlong-termcompressivememoryandlocalcausalattentionforef\\ue000cientlymod-elingbothlongandshort-rangecontextualdependencies.2.In\\ue000ni-attentionintroducesminimalchangetothestandardscaleddot-productatten-tionandsupportsplug-and-playcontinualpre-trainingandlong-contextadaptationbydesign.3.OurapproachenablesTransformerLLMstoscaletoin\\ue000nitelylongcontextwithaboundedmemoryandcomputeresourcebyprocessingextremelylonginputsinastreamingfashion.2MethodFigure2comparesourmodel,In\\ue000ni-Transformer,andTransformer-XL(Daietal.,2019).SimilartoTransformer-XL,In\\ue000ni-Transformeroperatesonasequenceofsegments.Wecomputethestandardcausaldot-productattentioncontextwithineachsegment.Sothedot-productattentioncomputationislocalinasensethatitcoversatotalNnumberoftokensofthecurrentsegmentwithindexS(Nisthesegmentlength).Thelocalattention(Daietal.,2019),however,discardstheattentionstatesoftheprevioussegmentwhenprocessingthenextone.InIn\\ue000ni-Transformers,insteadofleavingouttheoldKVattentionstates,weproposetoreusethemtomaintaintheentirecontexthistorywithacompressivememory.SoeachattentionlayerofIn\\ue000ni-Transformershasbothglobalcompressiveandlocal\\ue000ne-grainedstates.Wecallsuchanef\\ue000cientattentionmechanismIn\\ue000ni-attention,whichisillustratedinFigure1anddescribedformallyinthefollowingsections.2.1In\\ue000ni-attentionAsshownFigure1,ourIn\\ue000ni-attentioncomputesbothlocalandglobalcontextstatesandcombinethemforitsoutput.Similartomulti-headattention(MHA),itmaintainsHnumber2\\x0cPreprint.Underreview.Figure2:In\\ue000ni-Transformer(top)hasanentirecontexthistorywhereasTransformer-XL(bottom)discardsoldcontextssinceitcachestheKVstatesforthelastsegmentonly.ofparallelcompressivememoryperattentionlayer(Histhenumberofattentionheads)inadditiontothedot-productattention.2.1.1ScaledDot-productAttentionThemulti-headscaleddot-productattention(Vaswanietal.,2017),speciallyitsself-attentionvariant(Munkhdalaietal.,2016;Chengetal.,2016),hasbeenthemainbuildingblockinLLMs.TheMHA’sstrongcapabilitytomodelcontext-dependentdynamiccomputationanditsconveniencesoftemporalmaskinghavebeenleveragedextensivelyintheautoregressivegenerativemodels.AsingleheadinthevanillaMHAcomputesitsattentioncontextAdot∈IRN×dvaluefromsequenceofinputsegmentsX∈IRN×dmodelasfollows.First,itcomputesattentionquery,key,andvaluestates:K=XWK,V=XWVandQ=XWQ.(1)Here,WK∈IRdmodel×dkey,WV∈IRdmodel×dvalueandWQ∈IRdmodel×dkeyaretrainableprojectionmatrices.Then,theattentioncontextiscalculatedasaweightedaverageofallothervaluesasAdot=softmax\\ue000QKT√dmodel\\ue001V.(2)ForMHA,wecomputeHnumberofattentioncontextvectorsforeachsequenceelementinparallel,concatenatethemalongtheseconddimensionandthen\\ue000nallyprojecttheconcatenatedvectortothemodelspacetoobtainattentiontheoutput.2.1.2CompressiveMemoryInIn\\ue000ni-attention,insteadofcomputingnewmemoryentriesforcompressivememory,wereusethequery,keyandvaluestates(Q,KandV)fromthedot-productattentioncompu-tation.Thestatesharingandreusingbetweenthedot-productattentionandcompressivememorynotonlyenablesef\\ue000cientplug-in-playlong-contextadaptationbutalsospeedsuptrainingandinference.Similartothepriorwork(Munkhdalaietal.,2019),ourgoalistostorebindingsofkeyandvaluestatesinthecompressivememoryandretrievebyusingthequeryvectors.3\\x0cPreprint.Underreview.Whiletherearedifferentformsofcompressivememoryproposedintheliterature(Hop-\\ue000eld,1982;Kanerva,1988;Schlagetal.,2019;Munkhdalaietal.,2019),forsimplicityandcomputationalef\\ue000ciency,inthisworkweparameterizethememorywithanassociativematrix(Schlagetal.,2020).Thisapproachfurtherallowsustocastthememoryupdateandretrievalprocessaslinearattentionmechanism(Shenetal.,2018)andtoleveragestabletrainingtechniquesfromtherelatedmethods.Specially,weadopttheupdateruleandretrievalmechanismbyKatharopoulosetal.(2020)mainlyduetoitssimplicityandcompetitiveperformance.Memoryretrieval.InIn\\ue000ni-attention,weretrievenewcontentAmem∈IRN×dvaluefromthememoryMs−1∈IRdkey×dvaluebyusingthequeryQ∈IRN×dkeyas:Amem=σ(Q)Ms−1σ(Q)zs−1.(3)Here,σandzs−1∈IRdkeyareanonlinearactivationfunctionandanormalizationterm,respectively.Asthechoiceofthenon-linearityandthenormmethodiscrucialfortrainingstability,followingKatharopoulosetal.(2020)werecordasumoverallkeysasthenormal-izationtermzs−1anduseelement-wiseELU+1astheactivationfunction(Clevertetal.,2015).Memoryupdate.Oncetheretrievalisdone,weupdatethememoryandthenormalizationtermwiththenewKVentriesandobtainthenextstatesasMs←Ms−1+σ(K)TVandzs←zs−1+N∑t=1σ(Kt).(4)ThenewmemorystatesMsandzsarethenpassedtothenextsegmentS+1,buildinginarecurrenceineachattentionlayer.Therightsidetermσ(K)TVinEq.(4)isknownasanassociativebindingoperator(Smolensky,1990;Hebb,2005;Schlagetal.,2020).Inspiredbythesuccessofdeltarule(Munkhdalaietal.,2019;Schlagetal.,2020;2021),wehavealsoincorporateditintoourIn\\ue000ni-attention.Thedeltaruleattemptsaslightlyimprovedmemoryupdateby\\ue000rstretrievingexistingvalueentriesandsubtractingthemfromthenewvaluesbeforeapplyingtheassociativebindingsasnewupdate.Ms←Ms−1+σ(K)T(V−σ(K)Ms−1σ(K)zs−1).(5)Thisupdaterule(Linear+Delta)leavestheassociativematrixunmodi\\ue000ediftheKVbindingalreadyexistsinthememorywhilestilltrackingthesamenormalizationtermastheformerone(Linear)fornumericalstability.Long-termcontextinjection.WeaggregatethelocalattentionstateAdotandmemoryretrievedcontentAmemviaalearnedgatingscalarβ:A=sigmoid(β)⊙Amem+(1−sigmoid(β))⊙Adot.(6)Thisaddsonlyasinglescalarvalueastrainingparameterperheadwhileallowingalearnabletrade-offbetweenthelong-termandlocalinformation\\ue001owsinthemodel(Wuetal.,2022).SimilartothestandardMHA,forthemulti-headIn\\ue000ni-attentionwecomputeHnumberofcontextstatesinparallel,andconcatenateandprojectthemforthe\\ue000nalattentionoutputO∈IRN×dmodel:O=[A1;...AH]WO(7)whereWO∈IRH×dvalue×dmodelistrainableweights.2.2MemoryandEffectiveContextWindowOurIn\\ue000ni-Transformerenablesanunboundedcontextwindowwithaboundedmemoryfootprint.Toillustratethis,Table1liststheprevioussegment-levelmemorymodelswith4\\x0cPreprint.Underreview.ModelMemory(cache)footprintContextlengthMemoryupdateMemoryretrievalTransformer-XL(dkey+dvalue)×H×N×lN×lDiscardedDot-productattentionCompressiveTransformerdmodel×(c+N)×l(c×r+N)×lDiscardedDot-productattentionMemorizingTransformers(dkey+dvalue)×H×N×SN×SNonekNN+dot-productattentionRMTdmodel×p×l×2N×SDiscardedSoft-promptinputAutoCompressorsdmodel×p×(m+1)×lN×SDiscardedSoft-promptinputIn\\ue000ni-Transformersdkey×(dvalue+1)×H×lN×SIncrementalLinearattentionTable1:Transformermodelswithsegment-levelmemoryarecompared.Foreachmodel,thememorysizeandeffectivecontextlengtharede\\ue000nedintermsoftheirmodelparameters(N:inputsegmentlength,S:thenumberofsegments,l:thenumberoflayers,H:thenumberofattentionheads,c:CompressiveTransformermemorysize,r:compressionratio,p:thenumberofsoft-promptsummaryvectorsandm:summaryvectoraccumulationsteps).theircontext-memoryfootprintandeffectivecontextlengthde\\ue000nedintermsofmodelparametersandinputsegmentlength.In\\ue000ni-Transformerhasaconstantmemorycomplexityofdkey×dvalue+dkeyforstoringcompressedcontextinMsandzsforeachheadinsinglelayerwhilefortheothermodels,thecomplexitygrowsalongwiththesequencedimension-thememorycomplexitydependseitheronthecachesizeforTransformer-XL(Daietal.,2019),CompressiveTransformer(Raeetal.,2019)andMemorizingTransformers(Wuetal.,2022)oronthesoft-promptsizeforRTM(Bulatovetal.,2022)andAutoCompressors(Geetal.,2023).Figure3:TherearetwotypesofheadsemergedinIn\\ue000ni-attentionaftertraining:spe-cializedheadswithgatingscorenear0or1andmixerheadswithscorecloseto0.5.Thespecializedheadseitherprocesscontex-tualinformationviathelocalattentionmech-anismorretrievefromthecompressivemem-orywhereasthemixerheadsaggregatebothcurrentcontextualinformationandlong-termmemorycontenttogetherintosingleoutput.Transformer-XLcomputesattentionoverKVstatescachedfromthelastsegmentinadditiontothecurrentstates.Sincethisisdoneforeachlayer,Transformer-XLextendsthecontextwin-dowfromNtoN×ltokenswithanadditionalmemoryfootprintof(dkey+dvalue)×H×N×l.CompressiveTransformeraddsasecondcachetoTransformer-XLandstorescompressedrep-resentationsofpastsegmentactivations.SoitextendstheTransformer-XL’scontextwindowbyc×r×lbutstillhasalargecontext-memorycomplexity.Takingtheideafurther,Memoriz-ingTransformersopttostoretheentireKVstatesascontextforinputsequences.Sincethestor-agebecomesprohibitivelyexpensiveinthiscase,theyrestrictthecontextualcomputationtoasin-glelayeronly.ByutilizingafastkNNretriever,MemorizingTransformersthenbuildacontextwindowcoveringtheentiresequencehistoryoflengthN×Satanincreasedcostofstorage.OurexperimentsshowthatIn\\ue000ni-TransformerLMcanachievemorethan100xcompressionrateontopofMemorizingTransformerswhilefurtherimprovingtheperplexityscore.RMTandAutoCompressorsallowforapoten-tiallyin\\ue000nitecontextlengthsincetheycompresstheinputintosummaryvectorsandthenpassthemasextrasoft-promptinputsforthesubsequentsegments.However,inpracticethesuccessofthosetechniqueshighlydependsonthesizeofsoft-promptvectors.Namely,itisnecessarytoincreasethenumberofsoft-prompt(summary)vectorstoachieveabetterperformancewithAutoCompressors(Chevalieretal.,2023)andwiththat,thememoryandcomputecomplexitygrowquicklyresultingindiminishedef\\ue000ciency.ItwasalsoobservedinAutoCompressors(Chevalieretal.,2023)thatanef\\ue000cientcompressionobjectiveisneededfortrainingsuchpromptcompressiontechniques(Geetal.,2023).5\\x0cPreprint.Underreview.ModelMemorysize(comp.)XLcacheSegmentlengthPG19Arxiv-mathTransformer-XL50M(3.7x)2048204811.882.42MemorizingTransformers183M(1x)2048204811.372.26RMT2.5M(73x)None204813.272.55In\\ue000ni-Transformer(Linear)1.6M(114x)None20489.652.24In\\ue000ni-Transformer(Linear+Delta)1.6M(114x)None20489.672.23Table2:Long-contextlanguagemodelingresultsarecomparedintermsofaveragetoken-levelperplexity.Comp.denotescompressionratio.In\\ue000ni-Transformeroutperformsmemo-rizingtransformerswithmemorylengthof65Kandachieves114xcompressionratio.3ExperimentsWeevaluatedourIn\\ue000ni-Transformermodelsonbenchmarksinvolvingextremelylonginputsequences:long-contextlanguagemodeling,1Mlengthpasskeycontextblockretrievaland500Klengthbooksummarizationtasks.Forthelanguagemodelingbenchmark,wetrainourmodelsfromscratchwhileforthepasskeyandbooksummarizationtasks,wecontinuallypre-trainexistingLLMsinordertohighlightaplug-and-playlong-contextadaptationcapabilityofourapproach.3.1Long-contextLanguageModelingWetrainedandevaluatedsmallIn\\ue000ni-TransformermodelsonPG19(Raeetal.,2019)andArxiv-math(Wuetal.,2022)benchmarks.OursetupcloselyresemblesthatofMemorizingTransformers(Wuetal.,2022).Namely,allourmodelshave12layersand8attentionheadsofdimension128eachandFFNswithhiddenlayer4096.WesettheIn\\ue000ni-attentionsegmentlengthNto2048forallattentionlayersandtheinputsequencelengthto32768fortraining.ThisallowstheIn\\ue000ni-attentiontounrollover16stepsw.r.titscompressivememorystates.FortheRMTbaseline,weperformedseveralrunswithsummarypromptlengths50,100and150andsequencelengths4096,8196and32768.RMTwith100summaryvectorsgavethebestresultwhentrainedon8196lengthsequences.ThemainresultsfromthelanguagemodelingexperimentsaresummarizedinTable2.OurIn\\ue000ni-TransformeroutperformsbothTransformer-XL(Daietal.,2019)andMemorizingTransformers(Wuetal.,2022)baselineswhilemaintaining114xlessmemoryparametersthantheMemorizingTransformermodelwithavectorretrieval-basedKVmemorywithlengthof65Katits9thlayer.100Klengthtraining.Wefurtherincreasedthetrainingsequencelengthto100Kfrom32KandtrainedthemodelsonArxiv-mathdataset.100Ktrainingfurtherdecreasedtheperplexityscoreto2.21and2.20forLinearandLinear+Deltamodels.Gatingscorevisualization.Figure3visualizesthegatingscore,sigmoid(β)forthecompres-sivememoryforallattentionheadsineachlayer.TherearetwotypesofheadsemergedinIn\\ue000ni-attentionaftertraining:specializedheadswithagatingscorenear0or1andmixerheadswithascorecloseto0.5.ThespecializedheadseitherprocesscontextualinformationviathelocalattentioncomputationorretrievefromthecompressivememorywhereastheZero-shot32K128K256K512K1MIn\\ue000ni-Transformer(Linear)14/13/9811/14/1006/3/1006/7/998/6/98In\\ue000ni-Transformer(Linear+Delta)13/11/996/9/997/5/996/8/977/6/97FT(400steps)In\\ue000ni-Transformer(Linear)100/100/100100/100/100100/100/10097/99/10096/94/100In\\ue000ni-Transformer(Linear+Delta)100/100/100100/100/99100/100/99100/100/100100/100/100Table3:In\\ue000ni-Transformerssolvedthepasskeytaskwithupto1Mcontextlengthwhen\\ue000ne-tunedon5Klengthinputs.Wereporttoken-levelretrievalaccuracyforpasskeyshiddeninadifferentpart(start/middle/end)oflonginputswithlengths32Kto1M.6\\x0cPreprint.Underreview.ModelRouge-1Rouge-2Rouge-LOverallBART36.47.615.316.2BART+Unlimiformer36.88.315.716.9PRIMERA38.67.215.616.3PRIMERA+Unlimiformer37.98.216.317.2In\\ue000ni-Transformers(Linear)37.98.717.618.0In\\ue000ni-Transformers(Linear+Delta)40.08.817.918.5Table4:500Klengthbooksummarization(BookSum)results.TheBART,PRIMERAandUnlimiformerresultsarefromBertschetal.(2024).mixerheadsaggregatebothcurrentcontextualinformationandlong-termmemorycontenttogetherintoasingleoutput.Interestingly,eachlayerhasatleastasingleshort-rangehead,allowingaforward-propagationofinputsignalupuntiltheoutputlayer.Wealsoobservedaninterleavingoflongandshort-termcontentretrievalsthroughouttheforwardcomputation.3.2LLMContinualPre-trainingWeperformedalightweightcontinualpre-trainingforlong-contextadaptationofexistingLLMs.Thepre-trainingdataincludesthePG19andArxiv-mathcorpusaswellasC4text(Raffeletal.,2020)withlengthmorethan4Ktokens.ThesegmentlengthNwassetto2Kthroughoutourexperiments.1Mpasskeyretrievalbenchmark.WereplacedthevanillaMHAina1BLLMwithIn\\ue000ni-attentionandcontinuedtopre-trainoninputswithlengthof4K.Themodelwastrainedfor30Kstepswithbatchsizeof64before\\ue000ne-tuningonthepasskeyretrievaltask(Mohtashami&Jaggi,2024).Thepasskeytaskhidesarandomnumberintoalongtextandasksitbackatthemodeloutput.Thelengthofthedistractiontextisvariedbyrepeatingatextchunkmultipletimes.Thepreviouswork(Chenetal.,2023a)showedthata8BLLaMAmodelcansolvethetaskupto32Klengthwhen\\ue000ne-tunedwiththesame32KlengthinputswithPositionInterpolation.Wetakethischallengefurtherand\\ue000ne-tuneononly5Klengthinputstoteston1Mlengthregime.Figure4:In\\ue000ni-TransformersobtainbetterRougeoverallscoreswithmorebooktextpro-videdasinput.Table3reportsthetoken-levelaccuracyfortestsubsetswithinputlengthsrangingfrom32Kto1M.Foreachtestsubset,wecon-trolledthepositionofthepasskeysothatitiseitherlocatedaroundthebeginning,mid-dleortheendoftheinputsequence.Wereportedbothzero-shotaccuracyand\\ue000ne-tuningaccuracy.In\\ue000ni-Transformerssolvedthetaskwithupto1Mcontextlengthaf-ter\\ue000ne-tuningon5Klengthinputsfor400steps.500Klengthbooksummarization(Book-Sum).Wefurtherscaledourapproachbycontinuouslypre-traininga8BLLMmodelwith8Kinputlengthfor30Ksteps.Wethen\\ue000ne-tunedonabooksummarizationtask,BookSum(Kry´sci´nskietal.,2021)wherethegoalistogenerateasummaryofanentirebooktext.Wesettheinputlengthto32Kfor\\ue000ne-tuningandincreaseto500Kforevaluating.Weuseagenerationtemperatureof0.5andtopp=0.95andsetthenumberofdecodingstepsto1024togenerateasummaryofeachbook.7\\x0cPreprint.Underreview.Table4comparesourmodelagainsttheencoder-decodermodelsthatwerebuiltparticularlyforthesummarizationtask(Lewisetal.,2019;Xiaoetal.,2021)andtheirretrieval-basedlong-contextextension(Bertschetal.,2024).OurmodeloutperformsthepreviousbestresultsandachievesanewSOTAonBookSumbyprocessingtheentiretextfrombook.WehavealsoplottedtheoverallRougescoreonvalidationsplitofBookSumdatainFigure4.Thereisacleartrendshowingthatwithmoretextprovidedasinputfrombooks,OurIn\\ue000ni-Transformersimprovesitssummarizationperformancemetric.4RelatedWorkCompressivememory.Inspiredbytheplasticityinbiologicalneurons(Munkhdalai&Yu,2017a;Miconietal.,2018),compressivememoryapproachescastparameterizedfunctionsasmemorytostoreandretrieveinformation(Hinton&Plaut,1987;Schmidhuber,1992;Baetal.,2016;Munkhdalaietal.,2019).UnliketheTransformerKVmemoryarray(Vaswanietal.,2017;Wuetal.,2022),whichgrowswithinputsequencelength,compressivememorysystemsmaintainaconstantnumberofmemoryparametersforcomputationalef\\ue000ciency.Theparametersaremodi\\ue000edwithanupdateruletostoreinformation,whichisthenretrievedviaamemoryreadingmechanism(Gravesetal.,2014;Sukhbaataretal.,2015;Munkhdalai&Yu,2017b).Compressedinputrepresentationscanbeviewedasasummaryofpastsequenceseg-ments(Raeetal.,2019;Chevalieretal.,2023).Alongthisdirection,morerecentworkshavebeenutilizingaTransformerLLMitselftocompressinputsequenceforef\\ue000cientlong-contextmodeling(Bulatovetal.,2022;Chevalieretal.,2023;Geetal.,2023;Muetal.,2024).However,theprevioussegment-levelcompressionmethods,includingCompressiveTransformers(Raeetal.,2019)stilldiscardthememoryentriesofoldsegmentsinordertofreeupspaceforthenewones,limitingtheircontextwindowtothemostrecentsegments.ThisisincontrasttoourIn\\ue000ni-attentionthatcomputesincrementalmemoryupdatestoa\\ue000xedamountofmemoryparametersinarecurrentfashion.Long-contextcontinualpre-training.Thereisalineofworkthatextendsthedo-productattentionlayersandcontinuestotrainLLMsforlong-context(Xiongetal.,2023;Fuetal.,2024).Theattentionextensionsincludeincorporatingsparsityintotheattentionlayer(Chenetal.,2023b;Ratneretal.,2022;Mohtashami&Jaggi,2024)aswellasmanipulatingthepositionencodings(Chenetal.,2023a;Pengetal.,2023)Althoughthepositionencoding-basedmethodssuchaspositioninterpolationtechniques(Chenetal.,2023a)canbedataef\\ue000cientastheyonlyadjustthepositionalbiasintheattentionlayer,theyarestillcostlyforinference.Theattentionmechanismisalsopronetotheissuesofattentionsink(Xiaoetal.,2023)andlost-in-the-middle(Liuetal.,2024).Consequently,theystruggleinaregimewherecontextlengthislongerthanwhatwasobservedduringtraining(Pressetal.,2021;Kazemnejadetal.,2024).TheproposedIn\\ue000ni-attentionaddressesthoseissuesbyenablingasegment-levelstreamingcomputationoverlongsequenceswitha\\ue000xedlocalattentionwindow.OurIn\\ue000ni-Transformerssuccessfullyextrapolateto1Minputlengthregimeswhentrainedon32Kandeven5Klengthsequences.Ef\\ue000cientattention.Theef\\ue000cientattentiontechniquesattempttoimprovetheef\\ue000ciencyofthedot-productattentionwithanapproximationorasystem-leveloptimization.Multipledirectionshavebeenexploredfordifferentformsofef\\ue000cientattentionapproximation,includingsparsity-based(Childetal.,2019;Beltagyetal.,2020;Sukhbaataretal.,2021;Dingetal.,2023)andlinearattentionapproximation(Shenetal.,2018;Katharopoulosetal.,2020;Schlagetal.,2021).Amongthose,thelinearattentionvariantsarecloselyrelatedtotheassociativememorymatrix(Schlagetal.,2020;2021)andthemetalearnedneuralmemory(Munkhdalaietal.,2019),whereKVbindings(Smolensky,1990)arestoredinFast-Weights(Hinton&Plaut,1987;Schmidhuber,1992;Baetal.,2016)thataremodi\\ue000edinwithrespecttonewcontextualinformation.Morerecently,system-leveloptimizationtechniqueshavebeenproposedbyleveragingspeci\\ue000chardwarearchitecturetomaketheexactattentioncomputationmoreef\\ue000cient(Daoetal.,2022;Liuetal.,2023).8\\x0cPreprint.Underreview.5ConclusionAneffectivememorysystemiscrucialnotjustforcomprehendinglongcontextswithLLMs,butalsoforreasoning,planning,continualadaptationforfreshknowledge,andevenforlearninghowtolearn.Thisworkintroducesacloseintegrationofcompressivememorymod-uleintothevanilladot-productattentionlayer.Thissubtlebutcriticalmodi\\ue000cationtotheattentionlayerenablesLLMstoprocessin\\ue000nitelylongcontextswithboundedmemoryandcomputationresources.Weshowthatourapproachcannaturallyscaletoamillionlengthregimeofinputsequences,whileoutperformingthebaselinesonlong-contextlanguagemodelingbenchmarkandbooksummarizationtasks.Wealsodemonstrateapromisinglengthgeneralizationcapabilityofourapproach.1Bmodelthatwas\\ue000ne-tunedonupto5Ksequencelengthpasskeyinstancessolvedthe1Mlengthproblem.ReferencesRohanAnil,AndrewMDai,OrhanFirat,MelvinJohnson,DmitryLepikhin,AlexandrePassos,SiamakShakeri,EmanuelTaropa,PaigeBailey,ZhifengChen,etal.Palm2technicalreport.arXivpreprintarXiv:2305.10403,2023.JimmyBa,GeoffreyEHinton,VolodymyrMnih,JoelZLeibo,andCatalinIonescu.Usingfastweightstoattendtotherecentpast.Advancesinneuralinformationprocessingsystems,29,2016.DzmitryBahdanau,KyunghyunCho,andYoshuaBengio.Neuralmachinetranslationbyjointlylearningtoalignandtranslate.arXivpreprintarXiv:1409.0473,2014.IzBeltagy,MatthewEPeters,andArmanCohan.Longformer:Thelong-documenttrans-former.arXivpreprintarXiv:2004.05150,2020.AmandaBertsch,UriAlon,GrahamNeubig,andMatthewGormley.Unlimiformer:Long-rangetransformerswithunlimitedlengthinput.AdvancesinNeuralInformationProcessingSystems,36,2024.TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal.Languagemodelsarefew-shotlearners.Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.AydarBulatov,YuryKuratov,andMikhailBurtsev.Recurrentmemorytransformer.AdvancesinNeuralInformationProcessingSystems,35:11079–11091,2022.ShouyuanChen,ShermanWong,LiangjianChen,andYuandongTian.Extendingcon-textwindowoflargelanguagemodelsviapositionalinterpolation.arXivpreprintarXiv:2306.15595,2023a.YukangChen,ShengjuQian,HaotianTang,XinLai,ZhijianLiu,SongHan,andJiayaJia.Longlora:Ef\\ue000cient\\ue000ne-tuningoflong-contextlargelanguagemodels.arXivpreprintarXiv:2309.12307,2023b.JianpengCheng,LiDong,andMirellaLapata.Longshort-termmemory-networksformachinereading.arXivpreprintarXiv:1601.06733,2016.AlexisChevalier,AlexanderWettig,AnirudhAjith,andDanqiChen.Adaptinglanguagemodelstocompresscontexts.arXivpreprintarXiv:2305.14788,2023.RewonChild,ScottGray,AlecRadford,andIlyaSutskever.Generatinglongsequenceswithsparsetransformers.arXivpreprintarXiv:1904.10509,2019.Djork-Arn´eClevert,ThomasUnterthiner,andSeppHochreiter.Fastandaccuratedeepnetworklearningbyexponentiallinearunits(elus).arXivpreprintarXiv:1511.07289,2015.9\\x0cPreprint.Underreview.ZihangDai,ZhilinYang,YimingYang,JaimeCarbonell,QuocVLe,andRuslanSalakhut-dinov.Transformer-xl:Attentivelanguagemodelsbeyonda\\ue000xed-lengthcontext.arXivpreprintarXiv:1901.02860,2019.TriDao,DanFu,StefanoErmon,AtriRudra,andChristopherR´e.Flashattention:Fastandmemory-ef\\ue000cientexactattentionwithio-awareness.AdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.JiayuDing,ShumingMa,LiDong,XingxingZhang,ShaohanHuang,WenhuiWang,NanningZheng,andFuruWei.Longnet:Scalingtransformersto1,000,000,000tokens.arXivpreprintarXiv:2307.02486,2023.YaoFu,RameswarPanda,XinyaoNiu,XiangYue,HannanehHajishirzi,YoonKim,andHaoPeng.Dataengineeringforscalinglanguagemodelsto128kcontext.arXivpreprintarXiv:2402.10171,2024.TaoGe,JingHu,XunWang,Si-QingChen,andFuruWei.In-contextautoencoderforcontextcompressioninalargelanguagemodel.arXivpreprintarXiv:2307.06945,2023.AlexGraves,GregWayne,andIvoDanihelka.Neuralturingmachines.arXivpreprintarXiv:1410.5401,2014.DirkGroeneveld,IzBeltagy,PeteWalsh,AkshitaBhagia,RodneyKinney,OyvindTafjord,AnanyaHarshJha,HamishIvison,IanMagnusson,YizhongWang,etal.Olmo:Acceler-atingthescienceoflanguagemodels.arXivpreprintarXiv:2402.00838,2024.DonaldOldingHebb.Theorganizationofbehavior:Aneuropsychologicaltheory.Psychologypress,2005.GeoffreyEHintonandDavidCPlaut.Usingfastweightstodebluroldmemories.InProceedingsoftheninthannualconferenceoftheCognitiveScienceSociety,pp.177–186,1987.JohnJHop\\ue000eld.Neuralnetworksandphysicalsystemswithemergentcollectivecomputa-tionalabilities.Proceedingsofthenationalacademyofsciences,79(8):2554–2558,1982.PenttiKanerva.Sparsedistributedmemory.MITpress,1988.AngelosKatharopoulos,ApoorvVyas,NikolaosPappas,andFranc¸oisFleuret.Transformersarernns:Fastautoregressivetransformerswithlinearattention.InInternationalconferenceonmachinelearning,pp.5156–5165.PMLR,2020.AmirhosseinKazemnejad,InkitPadhi,KarthikeyanNatesanRamamurthy,PayelDas,andSivaReddy.Theimpactofpositionalencodingonlengthgeneralizationintransformers.AdvancesinNeuralInformationProcessingSystems,36,2024.WojciechKry´sci´nski,NazneenRajani,DivyanshAgarwal,CaimingXiong,andDragomirRadev.Booksum:Acollectionofdatasetsforlong-formnarrativesummarization.arXivpreprintarXiv:2105.08209,2021.MikeLewis,YinhanLiu,NamanGoyal,MarjanGhazvininejad,AbdelrahmanMohamed,OmerLevy,VesStoyanov,andLukeZettlemoyer.Bart:Denoisingsequence-to-sequencepre-trainingfornaturallanguagegeneration,translation,andcomprehension.arXivpreprintarXiv:1910.13461,2019.HaoLiu,MateiZaharia,andPieterAbbeel.Ringattentionwithblockwisetransformersfornear-in\\ue000nitecontext.arXivpreprintarXiv:2310.01889,2023.NelsonFLiu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,andPercyLiang.Lostinthemiddle:Howlanguagemodelsuselongcontexts.TransactionsoftheAssociationforComputationalLinguistics,12:157–173,2024.ThomasMiconi,KennethStanley,andJeffClune.Differentiableplasticity:trainingplasticneuralnetworkswithbackpropagation.InInternationalConferenceonMachineLearning,pp.3559–3568.PMLR,2018.10\\x0cPreprint.Underreview.AmirkeivanMohtashamiandMartinJaggi.Random-accessin\\ue000nitecontextlengthfortransformers.AdvancesinNeuralInformationProcessingSystems,36,2024.JesseMu,XiangLi,andNoahGoodman.Learningtocompresspromptswithgisttokens.AdvancesinNeuralInformationProcessingSystems,36,2024.TsendsurenMunkhdalaiandHongYu.Metanetworks.InInternationalconferenceonmachinelearning,pp.2554–2563.PMLR,2017a.TsendsurenMunkhdalaiandHongYu.Neuralsemanticencoders.InProceedingsoftheconference.AssociationforComputationalLinguistics.Meeting,volume1,pp.397.NIHPublicAccess,2017b.TsendsurenMunkhdalai,JohnPLalor,andHongYu.Citationanalysiswithneuralattentionmodels.InProceedingsoftheSeventhInternationalWorkshoponHealthTextMiningandInformationAnalysis,pp.69–77,2016.TsendsurenMunkhdalai,AlessandroSordoni,TongWang,andAdamTrischler.Metalearnedneuralmemory.AdvancesinNeuralInformationProcessingSystems,32,2019.BowenPeng,JeffreyQuesnelle,HongluFan,andEnricoShippole.Yarn:Ef\\ue000cientcontextwindowextensionoflargelanguagemodels.arXivpreprintarXiv:2309.00071,2023.ReinerPope,SholtoDouglas,AakankshaChowdhery,JacobDevlin,JamesBradbury,JonathanHeek,KefanXiao,ShivaniAgrawal,andJeffDean.Ef\\ue000cientlyscalingtrans-formerinference.ProceedingsofMachineLearningandSystems,5,2023.O\\ue000rPress,NoahASmith,andMikeLewis.Trainshort,testlong:Attentionwithlinearbiasesenablesinputlengthextrapolation.arXivpreprintarXiv:2108.12409,2021.JackWRae,AnnaPotapenko,SiddhantMJayakumar,andTimothyPLillicrap.Compressivetransformersforlong-rangesequencemodelling.arXivpreprintarXiv:1911.05507,2019.ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,andPeterJLiu.Exploringthelimitsoftransferlearningwithauni\\ue000edtext-to-texttransformer.TheJournalofMachineLearningResearch,21(1):5485–5551,2020.NirRatner,YoavLevine,YonatanBelinkov,OriRam,OmriAbend,EhudKarpas,AmnonShashua,KevinLeyton-Brown,andYoavShoham.Parallelcontextwindowsimprovein-contextlearningoflargelanguagemodels.arXivpreprintarXiv:2212.10947,2022.ImanolSchlag,PaulSmolensky,RolandFernandez,NebojsaJojic,J¨urgenSchmidhuber,andJianfengGao.Enhancingthetransformerwithexplicitrelationalencodingformathproblemsolving.arXivpreprintarXiv:1910.06611,2019.ImanolSchlag,TsendsurenMunkhdalai,andJ¨urgenSchmidhuber.Learningassociativeinferenceusingfastweightmemory.arXivpreprintarXiv:2011.07831,2020.ImanolSchlag,KazukiIrie,andJ¨urgenSchmidhuber.Lineartransformersaresecretlyfastweightprogrammers.InInternationalConferenceonMachineLearning,pp.9355–9366.PMLR,2021.J¨urgenSchmidhuber.Learningtocontrolfast-weightmemories:Analternativetodynamicrecurrentnetworks.NeuralComputation,4(1):131–139,1992.NoamShazeerandMitchellStern.Adafactor:Adaptivelearningrateswithsublinearmemorycost.InInternationalConferenceonMachineLearning,pp.4596–4604.PMLR,2018.ZhuoranShen,MingyuanZhang,HaiyuZhao,ShuaiYi,andHongshengLi.Ef\\ue000cientattention:Attentionwithlinearcomplexities.arXivpreprintarXiv:1812.01243,2018.PaulSmolensky.Tensorproductvariablebindingandtherepresentationofsymbolicstructuresinconnectionistsystems.Arti\\ue000cialintelligence,46(1-2):159–216,1990.11\\x0cPreprint.Underreview.SainbayarSukhbaatar,JasonWeston,RobFergus,etal.End-to-endmemorynetworks.Advancesinneuralinformationprocessingsystems,28,2015.SainbayarSukhbaatar,DaJu,SpencerPoff,StephenRoller,ArthurSzlam,JasonWeston,andAngelaFan.Notallmemoriesarecreatedequal:Learningtoforgetbyexpiring.InInternationalConferenceonMachineLearning,pp.9902–9912.PMLR,2021.HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal.Llama2:Openfoundationand\\ue000ne-tunedchatmodels.arXivpreprintarXiv:2307.09288,2023.AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.Attentionisallyouneed.Advancesinneuralinforma-tionprocessingsystems,30,2017.YuhuaiWu,MarkusNRabe,DeLesleyHutchins,andChristianSzegedy.Memorizingtransformers.arXivpreprintarXiv:2203.08913,2022.GuangxuanXiao,YuandongTian,BeidiChen,SongHan,andMikeLewis.Ef\\ue000cientstream-inglanguagemodelswithattentionsinks.arXivpreprintarXiv:2309.17453,2023.WenXiao,IzBeltagy,GiuseppeCarenini,andArmanCohan.Primera:Pyramid-basedmaskedsentencepre-trainingformulti-documentsummarization.arXivpreprintarXiv:2110.08499,2021.WenhanXiong,JingyuLiu,IgorMolybog,HejiaZhang,PrajjwalBhargava,RuiHou,LouisMartin,RashiRungta,KarthikAbinavSankararaman,BarlasOguz,etal.Effectivelong-contextscalingoffoundationmodels.arXivpreprintarXiv:2309.16039,2023.AAdditionalTrainingDetailsForthelong-contextlanguagemodelingtask,wesetthelearningrateto0.01byperform-ingsmallsearchovervaluesof0.003,0.005,0.01and0.03.WeusedtheAdafactoropti-mizer(Shazeer&Stern,2018)withlinearwarmupwith1000steps,followedbycosinedecay.Weappliedgradientcheckpointingaftereachsegmenttosavetosavememory.Thebatchsizewassetto64.FortheLLMexperiments,wesetthelearningrateto0.0001duringcontinualpre-trainingandtask\\ue000ne-tuning.BPasskeyRetrievalTaskBelowweshowedtheinputformatofthepasskeytask.Thereisanimportantinfohiddeninsidealotofirrelevanttext.Finditandmemorizethem.Iwillquizyouabouttheimportantinformationthere.Thegrassisgreen.Theskyisblue.Thesunisyellow.Herewego.Thereandbackagain.(repeatxtimes)Thepasskeyis9054.Rememberit.9054isthepasskey.Thegrassisgreen.Theskyisblue.Thesunisyellow.Herewego.Thereandackagain.(repeatytimes)Whatisthepasskey?Thepasskeyis12\\x0c', metadata={'source': 'paper.pdf'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1278, which is longer than the specified 500\n",
      "Created a chunk of size 5315, which is longer than the specified 500\n",
      "Created a chunk of size 2761, which is longer than the specified 500\n",
      "Created a chunk of size 3918, which is longer than the specified 500\n",
      "Created a chunk of size 3375, which is longer than the specified 500\n",
      "Created a chunk of size 16900, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "# Split PDF documents into chunks\n",
    "text_splitter = NLTKTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(pdf_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Preprint.Underreview.LeaveNoContextBehind:Ef\\ue000cientIn\\ue000niteContextTransformerswithIn\\ue000ni-attentionTsendsurenMunkhdalai,ManaalFaruquiandSiddharthGopalGoogletsendsuren@google.comAbstractThisworkintroducesanef\\ue000cientmethodtoscaleTransformer-basedLargeLanguageModels(LLMs)toin\\ue000nitelylonginputswithboundedmemoryandcomputation.Akeycomponentinourproposedapproachisanewat-tentiontechniquedubbedIn\\ue000ni-attention.TheIn\\ue000ni-attentionincorporatesacompressivememoryintothevanillaattentionmechanismandbuildsinbothmaskedlocalattentionandlong-termlinearattentionmechanismsinasingleTransformerblock.Wedemonstratetheeffectivenessofourapproachonlong-contextlanguagemodelingbenchmarks,1Msequencelengthpasskeycontextblockretrievaland500Klengthbooksummarizationtaskswith1Band8BLLMs.OurapproachintroducesminimalboundedmemoryparametersandenablesfaststreaminginferenceforLLMs.1IntroductionMemoryservesasacornerstoneofintelligence,asitenablesef\\ue000cientcomputationstailoredtospeci\\ue000ccontexts.However,Transformers(Vaswanietal.,2017)andTransformer-basedLLMs(Brownetal.,2020;Touvronetal.,2023;Aniletal.,2023;Groeneveldetal.,2024)haveaconstrainedcontext-dependentmemory,duetothenatureoftheattentionmechanism.Figure1:In\\ue000ni-attentionhasanaddi-tionalcompressivememorywithlinearattentionforprocessingin\\ue000nitelylongcontexts.', metadata={'source': 'paper.pdf'}),\n",
       " Document(page_content='{KV}s−1and{KV}sareatten-tionkeyandvaluesforcurrentandprevi-ousinputsegments,respectivelyandQstheattentionqueries.PEdenotespositionembeddings.TheattentionmechanisminTransformersex-hibitsquadraticcomplexityinbothmemoryfootprintandcomputationtime.Forexample,theattentionKey-Value(KV)stateshave3TBmemoryfootprintfora500Bmodelwithbatchsize512andcontextlength2048(Popeetal.,2023).Indeed,scalingLLMstolongersequences(i.e.1Mtokens)ischallengingwiththestandardTransformerarchitecturesandservinglongerandlongercontextmodelsbecomescostly\\ue000nan-cially.Compressivememorysystemspromisetobemorescalableandef\\ue000cientthantheattentionmechanismforextremelylongsequences(Kan-erva,1988;Munkhdalaietal.,2019).Insteadofusinganarraythatgrowswiththeinputse-quencelength,acompressivememoryprimarilymaintainsa\\ue000xednumberofparameterstostoreandrecallinformationwithaboundedstorageandcomputationcosts.Inthecompressivemem-ory,newinformationisaddedtothememorybychangingitsparameterswithanobjectivethatthisinformationcanberecoveredbacklateron.However,theLLMsintheircurrentstatehaveyettoseeaneffective,practicalcompres-sivememorytechniquethatbalancessimplicityalongwithquality.1arXiv:2404.07143v1  [cs.CL]  10 Apr 2024\\x0cPreprint.Underreview.Inthiswork,weintroduceanovelapproachthatenablesTransformerLLMstoeffectivelyprocessin\\ue000nitelylonginputswithboundedmemoryfootprintandcomputation.AkeycomponentinourproposedapproachisanewattentiontechniquedubbedIn\\ue000ni-attention(Figure1).TheIn\\ue000ni-attentionincorporatesacompressivememoryintothevanillaattentionmechanism(Bahdanauetal.,2014;Vaswanietal.,2017)andbuildsinbothmaskedlocalattentionandlong-termlinearattentionmechanismsinasingleTransformerblock.Suchasubtlebutcriticalmodi\\ue000cationtotheTransformerattentionlayerenablesanaturalextensionofexistingLLMstoin\\ue000nitelylongcontextsviacontinualpre-trainingand\\ue000ne-tuning.OurIn\\ue000ni-attentionreusesallthekey,valueandquerystatesofthestandardattentioncomputationforlong-termmemoryconsolidationandretrieval.WestoreoldKVstatesoftheattentioninthecompressivememory,insteadofdiscardingthemlikeinthestandardattentionmechanism.Wethenretrievethevaluesfromthememorybyusingtheattentionquerystateswhenprocessingsubsequentsequences.Tocomputethe\\ue000nalcontextualoutput,theIn\\ue000ni-attentionaggregatesthelong-termmemory-retrievedvaluesandthelocalattentioncontexts.Inourexperiments,weshowthatourapproachoutperformsbaselinemodelsonlong-contextlanguagemodelingbenchmarkswhilehaving114xcomprehensionratiointermsofmemorysize.Themodelachievesevenbetterperplexitywhentrainedwith100Ksequencelength.A1BLLMnaturallyscalesto1MsequencelengthandsolvesthepasskeyretrievaltaskwheninjectedwithIn\\ue000ni-attention.Finally,weshowthata8BmodelwithIn\\ue000ni-attentionreachesanewSOTAresultona500Klengthbooksummarizationtaskaftercontinualpre-trainingandtask\\ue000ne-tuning.Insummary,ourworkmakesthefollowingcontributions:1.Weintroduceapracticalandyetpowerfulattentionmechanism–In\\ue000ni-attentionwithlong-termcompressivememoryandlocalcausalattentionforef\\ue000cientlymod-elingbothlongandshort-rangecontextualdependencies.2.In\\ue000ni-attentionintroducesminimalchangetothestandardscaleddot-productatten-tionandsupportsplug-and-playcontinualpre-trainingandlong-contextadaptationbydesign.3.OurapproachenablesTransformerLLMstoscaletoin\\ue000nitelylongcontextwithaboundedmemoryandcomputeresourcebyprocessingextremelylonginputsinastreamingfashion.2MethodFigure2comparesourmodel,In\\ue000ni-Transformer,andTransformer-XL(Daietal.,2019).SimilartoTransformer-XL,In\\ue000ni-Transformeroperatesonasequenceofsegments.Wecomputethestandardcausaldot-productattentioncontextwithineachsegment.Sothedot-productattentioncomputationislocalinasensethatitcoversatotalNnumberoftokensofthecurrentsegmentwithindexS(Nisthesegmentlength).Thelocalattention(Daietal.,2019),however,discardstheattentionstatesoftheprevioussegmentwhenprocessingthenextone.InIn\\ue000ni-Transformers,insteadofleavingouttheoldKVattentionstates,weproposetoreusethemtomaintaintheentirecontexthistorywithacompressivememory.SoeachattentionlayerofIn\\ue000ni-Transformershasbothglobalcompressiveandlocal\\ue000ne-grainedstates.Wecallsuchanef\\ue000cientattentionmechanismIn\\ue000ni-attention,whichisillustratedinFigure1anddescribedformallyinthefollowingsections.2.1In\\ue000ni-attentionAsshownFigure1,ourIn\\ue000ni-attentioncomputesbothlocalandglobalcontextstatesandcombinethemforitsoutput.Similartomulti-headattention(MHA),itmaintainsHnumber2\\x0cPreprint.Underreview.Figure2:In\\ue000ni-Transformer(top)hasanentirecontexthistorywhereasTransformer-XL(bottom)discardsoldcontextssinceitcachestheKVstatesforthelastsegmentonly.ofparallelcompressivememoryperattentionlayer(Histhenumberofattentionheads)inadditiontothedot-productattention.2.1.1ScaledDot-productAttentionThemulti-headscaleddot-productattention(Vaswanietal.,2017),speciallyitsself-attentionvariant(Munkhdalaietal.,2016;Chengetal.,2016),hasbeenthemainbuildingblockinLLMs.TheMHA’sstrongcapabilitytomodelcontext-dependentdynamiccomputationanditsconveniencesoftemporalmaskinghavebeenleveragedextensivelyintheautoregressivegenerativemodels.AsingleheadinthevanillaMHAcomputesitsattentioncontextAdot∈IRN×dvaluefromsequenceofinputsegmentsX∈IRN×dmodelasfollows.First,itcomputesattentionquery,key,andvaluestates:K=XWK,V=XWVandQ=XWQ.(1)Here,WK∈IRdmodel×dkey,WV∈IRdmodel×dvalueandWQ∈IRdmodel×dkeyaretrainableprojectionmatrices.Then,theattentioncontextiscalculatedasaweightedaverageofallothervaluesasAdot=softmax\\ue000QKT√dmodel\\ue001V.', metadata={'source': 'paper.pdf'}),\n",
       " Document(page_content='(2)ForMHA,wecomputeHnumberofattentioncontextvectorsforeachsequenceelementinparallel,concatenatethemalongtheseconddimensionandthen\\ue000nallyprojecttheconcatenatedvectortothemodelspacetoobtainattentiontheoutput.2.1.2CompressiveMemoryInIn\\ue000ni-attention,insteadofcomputingnewmemoryentriesforcompressivememory,wereusethequery,keyandvaluestates(Q,KandV)fromthedot-productattentioncompu-tation.Thestatesharingandreusingbetweenthedot-productattentionandcompressivememorynotonlyenablesef\\ue000cientplug-in-playlong-contextadaptationbutalsospeedsuptrainingandinference.Similartothepriorwork(Munkhdalaietal.,2019),ourgoalistostorebindingsofkeyandvaluestatesinthecompressivememoryandretrievebyusingthequeryvectors.3\\x0cPreprint.Underreview.Whiletherearedifferentformsofcompressivememoryproposedintheliterature(Hop-\\ue000eld,1982;Kanerva,1988;Schlagetal.,2019;Munkhdalaietal.,2019),forsimplicityandcomputationalef\\ue000ciency,inthisworkweparameterizethememorywithanassociativematrix(Schlagetal.,2020).Thisapproachfurtherallowsustocastthememoryupdateandretrievalprocessaslinearattentionmechanism(Shenetal.,2018)andtoleveragestabletrainingtechniquesfromtherelatedmethods.Specially,weadopttheupdateruleandretrievalmechanismbyKatharopoulosetal.(2020)mainlyduetoitssimplicityandcompetitiveperformance.Memoryretrieval.InIn\\ue000ni-attention,weretrievenewcontentAmem∈IRN×dvaluefromthememoryMs−1∈IRdkey×dvaluebyusingthequeryQ∈IRN×dkeyas:Amem=σ(Q)Ms−1σ(Q)zs−1.(3)Here,σandzs−1∈IRdkeyareanonlinearactivationfunctionandanormalizationterm,respectively.Asthechoiceofthenon-linearityandthenormmethodiscrucialfortrainingstability,followingKatharopoulosetal.(2020)werecordasumoverallkeysasthenormal-izationtermzs−1anduseelement-wiseELU+1astheactivationfunction(Clevertetal.,2015).Memoryupdate.Oncetheretrievalisdone,weupdatethememoryandthenormalizationtermwiththenewKVentriesandobtainthenextstatesasMs←Ms−1+σ(K)TVandzs←zs−1+N∑t=1σ(Kt).(4)ThenewmemorystatesMsandzsarethenpassedtothenextsegmentS+1,buildinginarecurrenceineachattentionlayer.Therightsidetermσ(K)TVinEq.(4)isknownasanassociativebindingoperator(Smolensky,1990;Hebb,2005;Schlagetal.,2020).Inspiredbythesuccessofdeltarule(Munkhdalaietal.,2019;Schlagetal.,2020;2021),wehavealsoincorporateditintoourIn\\ue000ni-attention.Thedeltaruleattemptsaslightlyimprovedmemoryupdateby\\ue000rstretrievingexistingvalueentriesandsubtractingthemfromthenewvaluesbeforeapplyingtheassociativebindingsasnewupdate.Ms←Ms−1+σ(K)T(V−σ(K)Ms−1σ(K)zs−1).(5)Thisupdaterule(Linear+Delta)leavestheassociativematrixunmodi\\ue000ediftheKVbindingalreadyexistsinthememorywhilestilltrackingthesamenormalizationtermastheformerone(Linear)fornumericalstability.Long-termcontextinjection.WeaggregatethelocalattentionstateAdotandmemoryretrievedcontentAmemviaalearnedgatingscalarβ:A=sigmoid(β)⊙Amem+(1−sigmoid(β))⊙Adot.', metadata={'source': 'paper.pdf'}),\n",
       " Document(page_content='(6)Thisaddsonlyasinglescalarvalueastrainingparameterperheadwhileallowingalearnabletrade-offbetweenthelong-termandlocalinformation\\ue001owsinthemodel(Wuetal.,2022).SimilartothestandardMHA,forthemulti-headIn\\ue000ni-attentionwecomputeHnumberofcontextstatesinparallel,andconcatenateandprojectthemforthe\\ue000nalattentionoutputO∈IRN×dmodel:O=[A1;...AH]WO(7)whereWO∈IRH×dvalue×dmodelistrainableweights.2.2MemoryandEffectiveContextWindowOurIn\\ue000ni-Transformerenablesanunboundedcontextwindowwithaboundedmemoryfootprint.Toillustratethis,Table1liststheprevioussegment-levelmemorymodelswith4\\x0cPreprint.Underreview.ModelMemory(cache)footprintContextlengthMemoryupdateMemoryretrievalTransformer-XL(dkey+dvalue)×H×N×lN×lDiscardedDot-productattentionCompressiveTransformerdmodel×(c+N)×l(c×r+N)×lDiscardedDot-productattentionMemorizingTransformers(dkey+dvalue)×H×N×SN×SNonekNN+dot-productattentionRMTdmodel×p×l×2N×SDiscardedSoft-promptinputAutoCompressorsdmodel×p×(m+1)×lN×SDiscardedSoft-promptinputIn\\ue000ni-Transformersdkey×(dvalue+1)×H×lN×SIncrementalLinearattentionTable1:Transformermodelswithsegment-levelmemoryarecompared.Foreachmodel,thememorysizeandeffectivecontextlengtharede\\ue000nedintermsoftheirmodelparameters(N:inputsegmentlength,S:thenumberofsegments,l:thenumberoflayers,H:thenumberofattentionheads,c:CompressiveTransformermemorysize,r:compressionratio,p:thenumberofsoft-promptsummaryvectorsandm:summaryvectoraccumulationsteps).theircontext-memoryfootprintandeffectivecontextlengthde\\ue000nedintermsofmodelparametersandinputsegmentlength.In\\ue000ni-Transformerhasaconstantmemorycomplexityofdkey×dvalue+dkeyforstoringcompressedcontextinMsandzsforeachheadinsinglelayerwhilefortheothermodels,thecomplexitygrowsalongwiththesequencedimension-thememorycomplexitydependseitheronthecachesizeforTransformer-XL(Daietal.,2019),CompressiveTransformer(Raeetal.,2019)andMemorizingTransformers(Wuetal.,2022)oronthesoft-promptsizeforRTM(Bulatovetal.,2022)andAutoCompressors(Geetal.,2023).Figure3:TherearetwotypesofheadsemergedinIn\\ue000ni-attentionaftertraining:spe-cializedheadswithgatingscorenear0or1andmixerheadswithscorecloseto0.5.Thespecializedheadseitherprocesscontex-tualinformationviathelocalattentionmech-anismorretrievefromthecompressivemem-orywhereasthemixerheadsaggregatebothcurrentcontextualinformationandlong-termmemorycontenttogetherintosingleoutput.Transformer-XLcomputesattentionoverKVstatescachedfromthelastsegmentinadditiontothecurrentstates.Sincethisisdoneforeachlayer,Transformer-XLextendsthecontextwin-dowfromNtoN×ltokenswithanadditionalmemoryfootprintof(dkey+dvalue)×H×N×l.CompressiveTransformeraddsasecondcachetoTransformer-XLandstorescompressedrep-resentationsofpastsegmentactivations.SoitextendstheTransformer-XL’scontextwindowbyc×r×lbutstillhasalargecontext-memorycomplexity.Takingtheideafurther,Memoriz-ingTransformersopttostoretheentireKVstatesascontextforinputsequences.Sincethestor-agebecomesprohibitivelyexpensiveinthiscase,theyrestrictthecontextualcomputationtoasin-glelayeronly.ByutilizingafastkNNretriever,MemorizingTransformersthenbuildacontextwindowcoveringtheentiresequencehistoryoflengthN×Satanincreasedcostofstorage.OurexperimentsshowthatIn\\ue000ni-TransformerLMcanachievemorethan100xcompressionrateontopofMemorizingTransformerswhilefurtherimprovingtheperplexityscore.RMTandAutoCompressorsallowforapoten-tiallyin\\ue000nitecontextlengthsincetheycompresstheinputintosummaryvectorsandthenpassthemasextrasoft-promptinputsforthesubsequentsegments.However,inpracticethesuccessofthosetechniqueshighlydependsonthesizeofsoft-promptvectors.Namely,itisnecessarytoincreasethenumberofsoft-prompt(summary)vectorstoachieveabetterperformancewithAutoCompressors(Chevalieretal.,2023)andwiththat,thememoryandcomputecomplexitygrowquicklyresultingindiminishedef\\ue000ciency.ItwasalsoobservedinAutoCompressors(Chevalieretal.,2023)thatanef\\ue000cientcompressionobjectiveisneededfortrainingsuchpromptcompressiontechniques(Geetal.,2023).5\\x0cPreprint.Underreview.ModelMemorysize(comp.', metadata={'source': 'paper.pdf'}),\n",
       " Document(page_content=')XLcacheSegmentlengthPG19Arxiv-mathTransformer-XL50M(3.7x)2048204811.882.42MemorizingTransformers183M(1x)2048204811.372.26RMT2.5M(73x)None204813.272.55In\\ue000ni-Transformer(Linear)1.6M(114x)None20489.652.24In\\ue000ni-Transformer(Linear+Delta)1.6M(114x)None20489.672.23Table2:Long-contextlanguagemodelingresultsarecomparedintermsofaveragetoken-levelperplexity.Comp.denotescompressionratio.In\\ue000ni-Transformeroutperformsmemo-rizingtransformerswithmemorylengthof65Kandachieves114xcompressionratio.3ExperimentsWeevaluatedourIn\\ue000ni-Transformermodelsonbenchmarksinvolvingextremelylonginputsequences:long-contextlanguagemodeling,1Mlengthpasskeycontextblockretrievaland500Klengthbooksummarizationtasks.Forthelanguagemodelingbenchmark,wetrainourmodelsfromscratchwhileforthepasskeyandbooksummarizationtasks,wecontinuallypre-trainexistingLLMsinordertohighlightaplug-and-playlong-contextadaptationcapabilityofourapproach.3.1Long-contextLanguageModelingWetrainedandevaluatedsmallIn\\ue000ni-TransformermodelsonPG19(Raeetal.,2019)andArxiv-math(Wuetal.,2022)benchmarks.OursetupcloselyresemblesthatofMemorizingTransformers(Wuetal.,2022).Namely,allourmodelshave12layersand8attentionheadsofdimension128eachandFFNswithhiddenlayer4096.WesettheIn\\ue000ni-attentionsegmentlengthNto2048forallattentionlayersandtheinputsequencelengthto32768fortraining.ThisallowstheIn\\ue000ni-attentiontounrollover16stepsw.r.titscompressivememorystates.FortheRMTbaseline,weperformedseveralrunswithsummarypromptlengths50,100and150andsequencelengths4096,8196and32768.RMTwith100summaryvectorsgavethebestresultwhentrainedon8196lengthsequences.ThemainresultsfromthelanguagemodelingexperimentsaresummarizedinTable2.OurIn\\ue000ni-TransformeroutperformsbothTransformer-XL(Daietal.,2019)andMemorizingTransformers(Wuetal.,2022)baselineswhilemaintaining114xlessmemoryparametersthantheMemorizingTransformermodelwithavectorretrieval-basedKVmemorywithlengthof65Katits9thlayer.100Klengthtraining.Wefurtherincreasedthetrainingsequencelengthto100Kfrom32KandtrainedthemodelsonArxiv-mathdataset.100Ktrainingfurtherdecreasedtheperplexityscoreto2.21and2.20forLinearandLinear+Deltamodels.Gatingscorevisualization.Figure3visualizesthegatingscore,sigmoid(β)forthecompres-sivememoryforallattentionheadsineachlayer.TherearetwotypesofheadsemergedinIn\\ue000ni-attentionaftertraining:specializedheadswithagatingscorenear0or1andmixerheadswithascorecloseto0.5.ThespecializedheadseitherprocesscontextualinformationviathelocalattentioncomputationorretrievefromthecompressivememorywhereastheZero-shot32K128K256K512K1MIn\\ue000ni-Transformer(Linear)14/13/9811/14/1006/3/1006/7/998/6/98In\\ue000ni-Transformer(Linear+Delta)13/11/996/9/997/5/996/8/977/6/97FT(400steps)In\\ue000ni-Transformer(Linear)100/100/100100/100/100100/100/10097/99/10096/94/100In\\ue000ni-Transformer(Linear+Delta)100/100/100100/100/99100/100/99100/100/100100/100/100Table3:In\\ue000ni-Transformerssolvedthepasskeytaskwithupto1Mcontextlengthwhen\\ue000ne-tunedon5Klengthinputs.Wereporttoken-levelretrievalaccuracyforpasskeyshiddeninadifferentpart(start/middle/end)oflonginputswithlengths32Kto1M.6\\x0cPreprint.Underreview.ModelRouge-1Rouge-2Rouge-LOverallBART36.47.615.316.2BART+Unlimiformer36.88.315.716.9PRIMERA38.67.215.616.3PRIMERA+Unlimiformer37.98.216.317.2In\\ue000ni-Transformers(Linear)37.98.717.618.0In\\ue000ni-Transformers(Linear+Delta)40.08.817.918.5Table4:500Klengthbooksummarization(BookSum)results.TheBART,PRIMERAandUnlimiformerresultsarefromBertschetal.', metadata={'source': 'paper.pdf'}),\n",
       " Document(page_content='(2024).mixerheadsaggregatebothcurrentcontextualinformationandlong-termmemorycontenttogetherintoasingleoutput.Interestingly,eachlayerhasatleastasingleshort-rangehead,allowingaforward-propagationofinputsignalupuntiltheoutputlayer.Wealsoobservedaninterleavingoflongandshort-termcontentretrievalsthroughouttheforwardcomputation.3.2LLMContinualPre-trainingWeperformedalightweightcontinualpre-trainingforlong-contextadaptationofexistingLLMs.Thepre-trainingdataincludesthePG19andArxiv-mathcorpusaswellasC4text(Raffeletal.,2020)withlengthmorethan4Ktokens.ThesegmentlengthNwassetto2Kthroughoutourexperiments.1Mpasskeyretrievalbenchmark.WereplacedthevanillaMHAina1BLLMwithIn\\ue000ni-attentionandcontinuedtopre-trainoninputswithlengthof4K.Themodelwastrainedfor30Kstepswithbatchsizeof64before\\ue000ne-tuningonthepasskeyretrievaltask(Mohtashami&Jaggi,2024).Thepasskeytaskhidesarandomnumberintoalongtextandasksitbackatthemodeloutput.Thelengthofthedistractiontextisvariedbyrepeatingatextchunkmultipletimes.Thepreviouswork(Chenetal.,2023a)showedthata8BLLaMAmodelcansolvethetaskupto32Klengthwhen\\ue000ne-tunedwiththesame32KlengthinputswithPositionInterpolation.Wetakethischallengefurtherand\\ue000ne-tuneononly5Klengthinputstoteston1Mlengthregime.Figure4:In\\ue000ni-TransformersobtainbetterRougeoverallscoreswithmorebooktextpro-videdasinput.Table3reportsthetoken-levelaccuracyfortestsubsetswithinputlengthsrangingfrom32Kto1M.Foreachtestsubset,wecon-trolledthepositionofthepasskeysothatitiseitherlocatedaroundthebeginning,mid-dleortheendoftheinputsequence.Wereportedbothzero-shotaccuracyand\\ue000ne-tuningaccuracy.In\\ue000ni-Transformerssolvedthetaskwithupto1Mcontextlengthaf-ter\\ue000ne-tuningon5Klengthinputsfor400steps.500Klengthbooksummarization(Book-Sum).Wefurtherscaledourapproachbycontinuouslypre-traininga8BLLMmodelwith8Kinputlengthfor30Ksteps.Wethen\\ue000ne-tunedonabooksummarizationtask,BookSum(Kry´sci´nskietal.,2021)wherethegoalistogenerateasummaryofanentirebooktext.Wesettheinputlengthto32Kfor\\ue000ne-tuningandincreaseto500Kforevaluating.Weuseagenerationtemperatureof0.5andtopp=0.95andsetthenumberofdecodingstepsto1024togenerateasummaryofeachbook.7\\x0cPreprint.Underreview.Table4comparesourmodelagainsttheencoder-decodermodelsthatwerebuiltparticularlyforthesummarizationtask(Lewisetal.,2019;Xiaoetal.,2021)andtheirretrieval-basedlong-contextextension(Bertschetal.,2024).OurmodeloutperformsthepreviousbestresultsandachievesanewSOTAonBookSumbyprocessingtheentiretextfrombook.WehavealsoplottedtheoverallRougescoreonvalidationsplitofBookSumdatainFigure4.Thereisacleartrendshowingthatwithmoretextprovidedasinputfrombooks,OurIn\\ue000ni-Transformersimprovesitssummarizationperformancemetric.4RelatedWorkCompressivememory.Inspiredbytheplasticityinbiologicalneurons(Munkhdalai&Yu,2017a;Miconietal.,2018),compressivememoryapproachescastparameterizedfunctionsasmemorytostoreandretrieveinformation(Hinton&Plaut,1987;Schmidhuber,1992;Baetal.,2016;Munkhdalaietal.,2019).UnliketheTransformerKVmemoryarray(Vaswanietal.,2017;Wuetal.,2022),whichgrowswithinputsequencelength,compressivememorysystemsmaintainaconstantnumberofmemoryparametersforcomputationalef\\ue000ciency.Theparametersaremodi\\ue000edwithanupdateruletostoreinformation,whichisthenretrievedviaamemoryreadingmechanism(Gravesetal.,2014;Sukhbaataretal.,2015;Munkhdalai&Yu,2017b).Compressedinputrepresentationscanbeviewedasasummaryofpastsequenceseg-ments(Raeetal.,2019;Chevalieretal.,2023).Alongthisdirection,morerecentworkshavebeenutilizingaTransformerLLMitselftocompressinputsequenceforef\\ue000cientlong-contextmodeling(Bulatovetal.,2022;Chevalieretal.,2023;Geetal.,2023;Muetal.,2024).However,theprevioussegment-levelcompressionmethods,includingCompressiveTransformers(Raeetal.,2019)stilldiscardthememoryentriesofoldsegmentsinordertofreeupspaceforthenewones,limitingtheircontextwindowtothemostrecentsegments.ThisisincontrasttoourIn\\ue000ni-attentionthatcomputesincrementalmemoryupdatestoa\\ue000xedamountofmemoryparametersinarecurrentfashion.Long-contextcontinualpre-training.Thereisalineofworkthatextendsthedo-productattentionlayersandcontinuestotrainLLMsforlong-context(Xiongetal.,2023;Fuetal.,2024).Theattentionextensionsincludeincorporatingsparsityintotheattentionlayer(Chenetal.,2023b;Ratneretal.,2022;Mohtashami&Jaggi,2024)aswellasmanipulatingthepositionencodings(Chenetal.,2023a;Pengetal.,2023)Althoughthepositionencoding-basedmethodssuchaspositioninterpolationtechniques(Chenetal.,2023a)canbedataef\\ue000cientastheyonlyadjustthepositionalbiasintheattentionlayer,theyarestillcostlyforinference.Theattentionmechanismisalsopronetotheissuesofattentionsink(Xiaoetal.,2023)andlost-in-the-middle(Liuetal.,2024).Consequently,theystruggleinaregimewherecontextlengthislongerthanwhatwasobservedduringtraining(Pressetal.,2021;Kazemnejadetal.,2024).TheproposedIn\\ue000ni-attentionaddressesthoseissuesbyenablingasegment-levelstreamingcomputationoverlongsequenceswitha\\ue000xedlocalattentionwindow.OurIn\\ue000ni-Transformerssuccessfullyextrapolateto1Minputlengthregimeswhentrainedon32Kandeven5Klengthsequences.Ef\\ue000cientattention.Theef\\ue000cientattentiontechniquesattempttoimprovetheef\\ue000ciencyofthedot-productattentionwithanapproximationorasystem-leveloptimization.Multipledirectionshavebeenexploredfordifferentformsofef\\ue000cientattentionapproximation,includingsparsity-based(Childetal.,2019;Beltagyetal.,2020;Sukhbaataretal.,2021;Dingetal.,2023)andlinearattentionapproximation(Shenetal.,2018;Katharopoulosetal.,2020;Schlagetal.,2021).Amongthose,thelinearattentionvariantsarecloselyrelatedtotheassociativememorymatrix(Schlagetal.,2020;2021)andthemetalearnedneuralmemory(Munkhdalaietal.,2019),whereKVbindings(Smolensky,1990)arestoredinFast-Weights(Hinton&Plaut,1987;Schmidhuber,1992;Baetal.,2016)thataremodi\\ue000edinwithrespecttonewcontextualinformation.Morerecently,system-leveloptimizationtechniqueshavebeenproposedbyleveragingspeci\\ue000chardwarearchitecturetomaketheexactattentioncomputationmoreef\\ue000cient(Daoetal.,2022;Liuetal.,2023).8\\x0cPreprint.Underreview.5ConclusionAneffectivememorysystemiscrucialnotjustforcomprehendinglongcontextswithLLMs,butalsoforreasoning,planning,continualadaptationforfreshknowledge,andevenforlearninghowtolearn.Thisworkintroducesacloseintegrationofcompressivememorymod-uleintothevanilladot-productattentionlayer.Thissubtlebutcriticalmodi\\ue000cationtotheattentionlayerenablesLLMstoprocessin\\ue000nitelylongcontextswithboundedmemoryandcomputationresources.Weshowthatourapproachcannaturallyscaletoamillionlengthregimeofinputsequences,whileoutperformingthebaselinesonlong-contextlanguagemodelingbenchmarkandbooksummarizationtasks.Wealsodemonstrateapromisinglengthgeneralizationcapabilityofourapproach.1Bmodelthatwas\\ue000ne-tunedonupto5Ksequencelengthpasskeyinstancessolvedthe1Mlengthproblem.ReferencesRohanAnil,AndrewMDai,OrhanFirat,MelvinJohnson,DmitryLepikhin,AlexandrePassos,SiamakShakeri,EmanuelTaropa,PaigeBailey,ZhifengChen,etal.Palm2technicalreport.arXivpreprintarXiv:2305.10403,2023.JimmyBa,GeoffreyEHinton,VolodymyrMnih,JoelZLeibo,andCatalinIonescu.Usingfastweightstoattendtotherecentpast.Advancesinneuralinformationprocessingsystems,29,2016.DzmitryBahdanau,KyunghyunCho,andYoshuaBengio.Neuralmachinetranslationbyjointlylearningtoalignandtranslate.arXivpreprintarXiv:1409.0473,2014.IzBeltagy,MatthewEPeters,andArmanCohan.Longformer:Thelong-documenttrans-former.arXivpreprintarXiv:2004.05150,2020.AmandaBertsch,UriAlon,GrahamNeubig,andMatthewGormley.Unlimiformer:Long-rangetransformerswithunlimitedlengthinput.AdvancesinNeuralInformationProcessingSystems,36,2024.TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal.Languagemodelsarefew-shotlearners.Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.AydarBulatov,YuryKuratov,andMikhailBurtsev.Recurrentmemorytransformer.AdvancesinNeuralInformationProcessingSystems,35:11079–11091,2022.ShouyuanChen,ShermanWong,LiangjianChen,andYuandongTian.Extendingcon-textwindowoflargelanguagemodelsviapositionalinterpolation.arXivpreprintarXiv:2306.15595,2023a.YukangChen,ShengjuQian,HaotianTang,XinLai,ZhijianLiu,SongHan,andJiayaJia.Longlora:Ef\\ue000cient\\ue000ne-tuningoflong-contextlargelanguagemodels.arXivpreprintarXiv:2309.12307,2023b.JianpengCheng,LiDong,andMirellaLapata.Longshort-termmemory-networksformachinereading.arXivpreprintarXiv:1601.06733,2016.AlexisChevalier,AlexanderWettig,AnirudhAjith,andDanqiChen.Adaptinglanguagemodelstocompresscontexts.arXivpreprintarXiv:2305.14788,2023.RewonChild,ScottGray,AlecRadford,andIlyaSutskever.Generatinglongsequenceswithsparsetransformers.arXivpreprintarXiv:1904.10509,2019.Djork-Arn´eClevert,ThomasUnterthiner,andSeppHochreiter.Fastandaccuratedeepnetworklearningbyexponentiallinearunits(elus).arXivpreprintarXiv:1511.07289,2015.9\\x0cPreprint.Underreview.ZihangDai,ZhilinYang,YimingYang,JaimeCarbonell,QuocVLe,andRuslanSalakhut-dinov.Transformer-xl:Attentivelanguagemodelsbeyonda\\ue000xed-lengthcontext.arXivpreprintarXiv:1901.02860,2019.TriDao,DanFu,StefanoErmon,AtriRudra,andChristopherR´e.Flashattention:Fastandmemory-ef\\ue000cientexactattentionwithio-awareness.AdvancesinNeuralInformationProcessingSystems,35:16344–16359,2022.JiayuDing,ShumingMa,LiDong,XingxingZhang,ShaohanHuang,WenhuiWang,NanningZheng,andFuruWei.Longnet:Scalingtransformersto1,000,000,000tokens.arXivpreprintarXiv:2307.02486,2023.YaoFu,RameswarPanda,XinyaoNiu,XiangYue,HannanehHajishirzi,YoonKim,andHaoPeng.Dataengineeringforscalinglanguagemodelsto128kcontext.arXivpreprintarXiv:2402.10171,2024.TaoGe,JingHu,XunWang,Si-QingChen,andFuruWei.In-contextautoencoderforcontextcompressioninalargelanguagemodel.arXivpreprintarXiv:2307.06945,2023.AlexGraves,GregWayne,andIvoDanihelka.Neuralturingmachines.arXivpreprintarXiv:1410.5401,2014.DirkGroeneveld,IzBeltagy,PeteWalsh,AkshitaBhagia,RodneyKinney,OyvindTafjord,AnanyaHarshJha,HamishIvison,IanMagnusson,YizhongWang,etal.Olmo:Acceler-atingthescienceoflanguagemodels.arXivpreprintarXiv:2402.00838,2024.DonaldOldingHebb.Theorganizationofbehavior:Aneuropsychologicaltheory.Psychologypress,2005.GeoffreyEHintonandDavidCPlaut.Usingfastweightstodebluroldmemories.InProceedingsoftheninthannualconferenceoftheCognitiveScienceSociety,pp.177–186,1987.JohnJHop\\ue000eld.Neuralnetworksandphysicalsystemswithemergentcollectivecomputa-tionalabilities.Proceedingsofthenationalacademyofsciences,79(8):2554–2558,1982.PenttiKanerva.Sparsedistributedmemory.MITpress,1988.AngelosKatharopoulos,ApoorvVyas,NikolaosPappas,andFranc¸oisFleuret.Transformersarernns:Fastautoregressivetransformerswithlinearattention.InInternationalconferenceonmachinelearning,pp.5156–5165.PMLR,2020.AmirhosseinKazemnejad,InkitPadhi,KarthikeyanNatesanRamamurthy,PayelDas,andSivaReddy.Theimpactofpositionalencodingonlengthgeneralizationintransformers.AdvancesinNeuralInformationProcessingSystems,36,2024.WojciechKry´sci´nski,NazneenRajani,DivyanshAgarwal,CaimingXiong,andDragomirRadev.Booksum:Acollectionofdatasetsforlong-formnarrativesummarization.arXivpreprintarXiv:2105.08209,2021.MikeLewis,YinhanLiu,NamanGoyal,MarjanGhazvininejad,AbdelrahmanMohamed,OmerLevy,VesStoyanov,andLukeZettlemoyer.Bart:Denoisingsequence-to-sequencepre-trainingfornaturallanguagegeneration,translation,andcomprehension.arXivpreprintarXiv:1910.13461,2019.HaoLiu,MateiZaharia,andPieterAbbeel.Ringattentionwithblockwisetransformersfornear-in\\ue000nitecontext.arXivpreprintarXiv:2310.01889,2023.NelsonFLiu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,andPercyLiang.Lostinthemiddle:Howlanguagemodelsuselongcontexts.TransactionsoftheAssociationforComputationalLinguistics,12:157–173,2024.ThomasMiconi,KennethStanley,andJeffClune.Differentiableplasticity:trainingplasticneuralnetworkswithbackpropagation.InInternationalConferenceonMachineLearning,pp.3559–3568.PMLR,2018.10\\x0cPreprint.Underreview.AmirkeivanMohtashamiandMartinJaggi.Random-accessin\\ue000nitecontextlengthfortransformers.AdvancesinNeuralInformationProcessingSystems,36,2024.JesseMu,XiangLi,andNoahGoodman.Learningtocompresspromptswithgisttokens.AdvancesinNeuralInformationProcessingSystems,36,2024.TsendsurenMunkhdalaiandHongYu.Metanetworks.InInternationalconferenceonmachinelearning,pp.2554–2563.PMLR,2017a.TsendsurenMunkhdalaiandHongYu.Neuralsemanticencoders.InProceedingsoftheconference.AssociationforComputationalLinguistics.Meeting,volume1,pp.397.NIHPublicAccess,2017b.TsendsurenMunkhdalai,JohnPLalor,andHongYu.Citationanalysiswithneuralattentionmodels.InProceedingsoftheSeventhInternationalWorkshoponHealthTextMiningandInformationAnalysis,pp.69–77,2016.TsendsurenMunkhdalai,AlessandroSordoni,TongWang,andAdamTrischler.Metalearnedneuralmemory.AdvancesinNeuralInformationProcessingSystems,32,2019.BowenPeng,JeffreyQuesnelle,HongluFan,andEnricoShippole.Yarn:Ef\\ue000cientcontextwindowextensionoflargelanguagemodels.arXivpreprintarXiv:2309.00071,2023.ReinerPope,SholtoDouglas,AakankshaChowdhery,JacobDevlin,JamesBradbury,JonathanHeek,KefanXiao,ShivaniAgrawal,andJeffDean.Ef\\ue000cientlyscalingtrans-formerinference.ProceedingsofMachineLearningandSystems,5,2023.O\\ue000rPress,NoahASmith,andMikeLewis.Trainshort,testlong:Attentionwithlinearbiasesenablesinputlengthextrapolation.arXivpreprintarXiv:2108.12409,2021.JackWRae,AnnaPotapenko,SiddhantMJayakumar,andTimothyPLillicrap.Compressivetransformersforlong-rangesequencemodelling.arXivpreprintarXiv:1911.05507,2019.ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,andPeterJLiu.Exploringthelimitsoftransferlearningwithauni\\ue000edtext-to-texttransformer.TheJournalofMachineLearningResearch,21(1):5485–5551,2020.NirRatner,YoavLevine,YonatanBelinkov,OriRam,OmriAbend,EhudKarpas,AmnonShashua,KevinLeyton-Brown,andYoavShoham.Parallelcontextwindowsimprovein-contextlearningoflargelanguagemodels.arXivpreprintarXiv:2212.10947,2022.ImanolSchlag,PaulSmolensky,RolandFernandez,NebojsaJojic,J¨urgenSchmidhuber,andJianfengGao.Enhancingthetransformerwithexplicitrelationalencodingformathproblemsolving.arXivpreprintarXiv:1910.06611,2019.ImanolSchlag,TsendsurenMunkhdalai,andJ¨urgenSchmidhuber.Learningassociativeinferenceusingfastweightmemory.arXivpreprintarXiv:2011.07831,2020.ImanolSchlag,KazukiIrie,andJ¨urgenSchmidhuber.Lineartransformersaresecretlyfastweightprogrammers.InInternationalConferenceonMachineLearning,pp.9355–9366.PMLR,2021.J¨urgenSchmidhuber.Learningtocontrolfast-weightmemories:Analternativetodynamicrecurrentnetworks.NeuralComputation,4(1):131–139,1992.NoamShazeerandMitchellStern.Adafactor:Adaptivelearningrateswithsublinearmemorycost.InInternationalConferenceonMachineLearning,pp.4596–4604.PMLR,2018.ZhuoranShen,MingyuanZhang,HaiyuZhao,ShuaiYi,andHongshengLi.Ef\\ue000cientattention:Attentionwithlinearcomplexities.arXivpreprintarXiv:1812.01243,2018.PaulSmolensky.Tensorproductvariablebindingandtherepresentationofsymbolicstructuresinconnectionistsystems.Arti\\ue000cialintelligence,46(1-2):159–216,1990.11\\x0cPreprint.Underreview.SainbayarSukhbaatar,JasonWeston,RobFergus,etal.End-to-endmemorynetworks.Advancesinneuralinformationprocessingsystems,28,2015.SainbayarSukhbaatar,DaJu,SpencerPoff,StephenRoller,ArthurSzlam,JasonWeston,andAngelaFan.Notallmemoriesarecreatedequal:Learningtoforgetbyexpiring.InInternationalConferenceonMachineLearning,pp.9902–9912.PMLR,2021.HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal.Llama2:Openfoundationand\\ue000ne-tunedchatmodels.arXivpreprintarXiv:2307.09288,2023.AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.Attentionisallyouneed.Advancesinneuralinforma-tionprocessingsystems,30,2017.YuhuaiWu,MarkusNRabe,DeLesleyHutchins,andChristianSzegedy.Memorizingtransformers.arXivpreprintarXiv:2203.08913,2022.GuangxuanXiao,YuandongTian,BeidiChen,SongHan,andMikeLewis.Ef\\ue000cientstream-inglanguagemodelswithattentionsinks.arXivpreprintarXiv:2309.17453,2023.WenXiao,IzBeltagy,GiuseppeCarenini,andArmanCohan.Primera:Pyramid-basedmaskedsentencepre-trainingformulti-documentsummarization.arXivpreprintarXiv:2110.08499,2021.WenhanXiong,JingyuLiu,IgorMolybog,HejiaZhang,PrajjwalBhargava,RuiHou,LouisMartin,RashiRungta,KarthikAbinavSankararaman,BarlasOguz,etal.Effectivelong-contextscalingoffoundationmodels.arXivpreprintarXiv:2309.16039,2023.AAdditionalTrainingDetailsForthelong-contextlanguagemodelingtask,wesetthelearningrateto0.01byperform-ingsmallsearchovervaluesof0.003,0.005,0.01and0.03.WeusedtheAdafactoropti-mizer(Shazeer&Stern,2018)withlinearwarmupwith1000steps,followedbycosinedecay.Weappliedgradientcheckpointingaftereachsegmenttosavetosavememory.Thebatchsizewassetto64.FortheLLMexperiments,wesetthelearningrateto0.0001duringcontinualpre-trainingandtask\\ue000ne-tuning.BPasskeyRetrievalTaskBelowweshowedtheinputformatofthepasskeytask.Thereisanimportantinfohiddeninsidealotofirrelevanttext.Finditandmemorizethem.Iwillquizyouabouttheimportantinformationthere.Thegrassisgreen.Theskyisblue.Thesunisyellow.Herewego.Thereandbackagain.(repeatxtimes)Thepasskeyis9054.Rememberit.9054isthepasskey.Thegrassisgreen.Theskyisblue.Thesunisyellow.Herewego.Thereandackagain.', metadata={'source': 'paper.pdf'}),\n",
       " Document(page_content='(repeatytimes)Whatisthepasskey?Thepasskeyis12', metadata={'source': 'paper.pdf'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for the chunks\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(google_api_key=KEY, model=\"models/embedding-001\")\n",
    "db = Chroma.from_documents(chunks, embedding_model, persist_directory=\"my_rag_chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoogleGenerativeAIEmbeddings(model='models/embedding-001', task_type=None, google_api_key=SecretStr('**********'), credentials=None, client_options=None, transport=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x196ad6af970>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define retriever and format docs function\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 7})\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are Defining RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | chat_template\n",
    "    | chat_model\n",
    "    | output_parser\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is about page1 in paper\n",
      "\n",
      "\n",
      "## Page 1 of the Paper: Introducing Infinite Context with Inni-attention\n",
      "\n",
      "Page 1 of the paper lays the groundwork for the proposed **Inni-attention** mechanism and its application within **Inni-Transformers** to address the limitations of traditional Transformer models when dealing with long sequences. Here's a breakdown of the key points:\n",
      "\n",
      "**Motivation:**\n",
      "\n",
      "* **Memory limitations of Transformers:** Traditional Transformers, while powerful, struggle with long sequences due to the quadratic complexity of the attention mechanism in terms of memory and computation. This makes scaling them to longer contexts (e.g., 1M tokens) challenging and expensive.\n",
      "* **Need for efficient memory systems:** Compressive memory systems offer a potential solution by maintaining a fixed number of parameters to store and recall information, enabling more scalable and efficient processing of long sequences.\n",
      "\n",
      "**Proposed Approach:**\n",
      "\n",
      "* **Inni-attention:** This novel attention technique integrates a compressive memory module into the standard attention layer of Transformers. It combines both **masked local attention** for focusing on recent context and **long-term linear attention** for accessing information from the entire history.\n",
      "* **Inni-Transformers:** By incorporating Inni-attention, these models can efficiently process infinitely long inputs with bounded memory and computation resources.\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **Reusing attention states:** Instead of discarding previous attention states, Inni-attention stores them in the compressive memory for future retrieval, enabling efficient context history maintenance.\n",
      "* **Bounded memory footprint:** The memory complexity of Inni-attention remains constant regardless of the input sequence length, making it more scalable than traditional attention mechanisms.\n",
      "* **Plug-and-play adaptation:** Inni-attention is designed for seamless integration into existing LLMs through continual pre-training and fine-tuning.\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "* **Improved performance:** Outperforms baseline models on long-context language modeling benchmarks with a significant compression ratio.\n",
      "* **Scalability to long sequences:** Enables LLMs to handle sequences of up to 1 million tokens, surpassing the limitations of traditional models.\n",
      "* **Efficient inference:** Allows for fast streaming computation over long sequences, reducing the computational burden during inference.\n",
      "\n",
      "**Overall, page 1 sets the stage for a novel approach to long-context modeling with LLMs, emphasizing the efficiency and scalability of Inni-attention and Inni-Transformers.** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Invoke RAG chain to generate answer\n",
    "user_input = input(\"enter user query\")\n",
    "response = rag_chain.invoke(user_input)\n",
    "# Display answer\n",
    "print(user_input)\n",
    "print(\"\\n\")\n",
    "print(response)\n",
    "\n",
    "\n",
    "#hence we can see now below text is in normal way no attaractive with no Bold etc\n",
    "\n",
    "#it took me 26 seconds to run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Page 1 of the Paper: Introducing Infinite Context with Inni-attention\n",
       "\n",
       "Page 1 of the paper lays the groundwork for the proposed **Inni-attention** mechanism and its application within **Inni-Transformers** to address the limitations of traditional Transformer models when dealing with long sequences. Here's a breakdown of the key points:\n",
       "\n",
       "**Motivation:**\n",
       "\n",
       "* **Memory limitations of Transformers:** Traditional Transformers, while powerful, struggle with long sequences due to the quadratic complexity of the attention mechanism in terms of memory and computation. This makes scaling them to longer contexts (e.g., 1M tokens) challenging and expensive.\n",
       "* **Need for efficient memory systems:** Compressive memory systems offer a potential solution by maintaining a fixed number of parameters to store and recall information, enabling more scalable and efficient processing of long sequences.\n",
       "\n",
       "**Proposed Approach:**\n",
       "\n",
       "* **Inni-attention:** This novel attention technique integrates a compressive memory module into the standard attention layer of Transformers. It combines both **masked local attention** for focusing on recent context and **long-term linear attention** for accessing information from the entire history.\n",
       "* **Inni-Transformers:** By incorporating Inni-attention, these models can efficiently process infinitely long inputs with bounded memory and computation resources.\n",
       "\n",
       "**Key Features:**\n",
       "\n",
       "* **Reusing attention states:** Instead of discarding previous attention states, Inni-attention stores them in the compressive memory for future retrieval, enabling efficient context history maintenance.\n",
       "* **Bounded memory footprint:** The memory complexity of Inni-attention remains constant regardless of the input sequence length, making it more scalable than traditional attention mechanisms.\n",
       "* **Plug-and-play adaptation:** Inni-attention is designed for seamless integration into existing LLMs through continual pre-training and fine-tuning.\n",
       "\n",
       "**Benefits:**\n",
       "\n",
       "* **Improved performance:** Outperforms baseline models on long-context language modeling benchmarks with a significant compression ratio.\n",
       "* **Scalability to long sequences:** Enables LLMs to handle sequences of up to 1 million tokens, surpassing the limitations of traditional models.\n",
       "* **Efficient inference:** Allows for fast streaming computation over long sequences, reducing the computational burden during inference.\n",
       "\n",
       "**Overall, page 1 sets the stage for a novel approach to long-context modeling with LLMs, emphasizing the efficiency and scalability of Inni-attention and Inni-Transformers.** \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://ipython.org/\n",
    "\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "md(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## RAG: Retrieval-Augmented Generation\n",
       "\n",
       "RAG, which stands for **Retrieval-Augmented Generation**, is a framework that enhances the capabilities of language models by incorporating information retrieval techniques. Unlike standard language models that rely solely on their internal knowledge, RAG models can access and process external information during text generation. This allows them to generate more comprehensive and informative responses, especially when dealing with open ended, challenging, or specific questions.\n",
       "\n",
       "Here's how RAG works:\n",
       "\n",
       "1. **Document Retrieval:** Given a user query, the model first retrieves relevant documents from a large external database (e.g., Wikipedia, news articles, books). This retrieval step can utilize various methods like keyword search or dense vector representations.\n",
       "2. **Document Processing:** The retrieved documents are then processed and encoded into a format that the language model can understand. This often involves techniques like summarization or extracting key information.\n",
       "3. **Generation with Retrieval Context:** The language model generates text based on both the user query and the processed information from the retrieved documents. This allows the model to incorporate external knowledge and generate more informative responses.\n",
       "\n",
       "**Benefits of RAG:**\n",
       "\n",
       "* **Improved Factual Accuracy:** By accessing external information, RAG models can provide more accurate and up-to-date responses, especially for factual queries.\n",
       "* **Enhanced Knowledge Coverage:** RAG models can access a vast amount of information beyond their internal knowledge, allowing them to handle a wider range of topics and questions.\n",
       "* **Contextual Understanding:** The retrieved documents provide context for the user query, enabling the model to better understand the user's intent and generate more relevant responses.\n",
       "\n",
       "**Examples of RAG Applications:**\n",
       "\n",
       "* **Question Answering:** RAG models can answer open ended or complex questions by retrieving relevant documents and extracting the necessary information.\n",
       "* **Summarization:** RAG models can generate summaries of factual topics by combining information from multiple sources.\n",
       "* **Chatbots:** RAG models can be used to build more knowledgeable and engaging chatbots that can access and process information from the real world.\n",
       "\n",
       "**Limitations of RAG:**\n",
       "\n",
       "* **Retrieval Quality:** The effectiveness of RAG models heavily depends on the quality of the retrieved documents. Irrelevant or inaccurate documents can lead to incorrect or misleading responses.\n",
       "* **Computational Cost:** The retrieval and processing of external information can be computationally expensive, especially for large databases.\n",
       "* **Bias and Fairness:** The external information sources used by RAG models can contain biases or misinformation, which can be reflected in the generated text.\n",
       "\n",
       "Overall, RAG is a powerful framework that combines the strengths of language models and information retrieval techniques. It has the potential to significantly improve the capabilities of language models and enable them to generate more informative, accurate, and contextually relevant responses. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now usinf Ipython\n",
    "#we can create beautiful context:\n",
    "\n",
    "# Invoke RAG chain to generate answer\n",
    "user_input = input(\"enter user query\")\n",
    "response = rag_chain.invoke(user_input)\n",
    "# Display answer\n",
    "#print(user_input)\n",
    "print(\"\\n\")\n",
    "#print(response)\n",
    "\n",
    "\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "md(user_input)\n",
    "md(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
